# XAI


## Overview
This project focuses on introducing a novel evaluation metric for Explainable Artificial Intelligence (XAI), called **Predictive Explanation Concordance (PEC)**. PEC aims to assess the alignment between model explanations and predictive behavior, offering a more comprehensive evaluation of XAI methods, such as SHAP and LIME, especially for graph data. The primary goal is to improve the interpretability and trustworthiness of AI systems by combining fidelity, sparsity, stability, and robustness.

## Key Features
- Introduction of the **PEC** evaluation metric for XAI.
- Integration of **fidelity**, **sparsity**, **stability**, and **robustness** into a unified framework.
- XAI methods **Lime**, **PyXAI**, **SHAP**, **GNNExpaliner**
- Empirical experiments demonstrating PEC's effectiveness.

## Project Structure
- `XAI/`: Scripts for running experiments and evaluating the PEC metric.


## Installation
To run this project locally:

Clone the repository:
   ```bash
   git clone https://github.com/anonymouaccount/XAI.git
